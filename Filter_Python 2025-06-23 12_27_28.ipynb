{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a004fac0-9e44-418f-b420-5c316d7eb45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filter Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7f9f82-924e-4a48-8d7f-c7388f1c09be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----+-----+\n|firstname|  lastname|country|state|  sal|\n+---------+----------+-------+-----+-----+\n|     raja|pushpa    |    USA|     |30000|\n|    priya|    pushpa|    USA|     |29900|\n|  Karthik|      Subu|    USA|   CA| 6000|\n|    James|     Smith|    USA|   FL|20000|\n|   Martin|   Jones  |    USA|   CA| 3000|\n|      Sam|  Anderson|     UK|  LND| 8000|\n|    Maria|   Patrick|     UK|  MCR| 7000|\n|    Robet|     Bevon|     UK|  LND| 3500|\n|    Maria|  Anderson|     UK|  MCR| 3000|\n+---------+----------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "staticlist = [(\" raja\", \"pushpa    \", \"USA\",\"\",30000),\n",
    "            (\" priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\", \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones  \", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c21beb5-fb98-40de-b7ea-3aa0d29663b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n+---------+--------+-------+-----+-----+\n\n+---------+--------+-------+-----+----+\n|firstname|lastname|country|state| sal|\n+---------+--------+-------+-----+----+\n|  Karthik|    Subu|    USA|   CA|6000|\n+---------+--------+-------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#place  a filter where salary is above 5000 #single column filter\n",
    "df_filter1 = df.filter(\"sal > 5000\")\n",
    "df_filter1.show()\n",
    "#multiple column filter\n",
    "df_filter2 = df.filter(\"sal > 5000 and state = 'CA'\")\n",
    "df_filter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ace44ba-7261-466b-8dda-053c86f8e3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5871695978945167>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_filter2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39msal \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m5000\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m df_filter2\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'df' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5871695978945167>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_filter2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39msal \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m5000\u001B[39m)\n\u001B[1;32m      2\u001B[0m df_filter2\u001B[38;5;241m.\u001B[39mshow()\n",
        "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_filter2 = df.filter(df.sal > 5000)\n",
    "df_filter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8691c358-dfe9-4948-aed5-c83c9e8cea1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|Firstname|Lastname|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|     raja|  pushpa|    USA|     | 30000|\n|    priya|  pushpa|    USA|     | 29900|\n|  Karthik|    Subu|    USA|   CA|  6000|\n|    James|   Smith|    USA|   FL| 20000|\n|   Martin|   Jones|    USA|   CA|  3000|\n|      Sam|Anderson|     UK|  LND|  8000|\n|    Maria| Patrick|     UK|  MCR|  7000|\n|    Robet|   Bevon|     UK|  LND|  3500|\n|    Maria|Anderson|     UK|  MCR|  3000|\n+---------+--------+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "staticlist = [(\" raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\" priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\", \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "StructSchema=StructType([\n",
    "    StructField(\"Firstname\",StringType(),False),\n",
    "    StructField(\"Lastname\",StringType(),True),\n",
    "    StructField(\"Country\",StringType(),False),\n",
    "    StructField(\"State\",StringType(),True),\n",
    "    StructField(\"Salary\",LongType(),False)])\n",
    "df_struct=spark.createDataFrame(data=staticlist,schema=StructSchema)\n",
    "df_struct.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef95c27-7e6f-4cde-9769-bd0078b0aa55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|Firstname|Lastname|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|     raja|  pushpa|    USA|     | 30000|\n+---------+--------+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#filter using Dataframe method\n",
    "df_filter2 = df_struct.filter(df_struct.Salary >= 30000)\n",
    "df_filter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a10fbd-efd1-4e13-9252-06d06766344d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|Firstname|Lastname|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|      Sam|Anderson|     UK|  LND|  8000|\n|    Maria| Patrick|     UK|  MCR|  7000|\n+---------+--------+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#option2: Multiple Column filters\n",
    "# when using dataframe AND will be replaces with \"&\" symbol, OR will be replace with \"|\" symbol and  \"=\" will be replaced with \"==\" symbol\n",
    "df_filter3 = df_struct.filter((df_struct.Salary > 5000) & (df_struct.Country == 'UK'))\n",
    "df_filter3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508c4b1e-6438-4194-9463-f19a25569e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|Firstname|Lastname|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|     raja|  pushpa|    USA|     | 30000|\n|    priya|  pushpa|    USA|     | 29900|\n|      Sam|Anderson|     UK|  LND|  8000|\n+---------+--------+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Multiple colums and multiple conditions\n",
    "# If country is US then salary > 20000, If Country is not US then salary > 5000\n",
    "\n",
    "df_filter4 = df_struct.filter(\n",
    "    ((df_struct.Country == 'USA') & (df_struct.Salary > 20000))\n",
    "    |\n",
    "    ((df_struct.Country != 'USA') & (df_struct.Salary > 7000))\n",
    ")\n",
    "\n",
    "df_filter4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de973e1-d1e6-46e4-a7d4-0c9b2eee14b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+---------+---------+---------+\n|Firstname|Lastname|Country|State|Salary|UpperCase|LowerCase|TitleCase|\n+---------+--------+-------+-----+------+---------+---------+---------+\n|     raja|  pushpa|    USA|     | 30000|     RAJA|     raja|     Raja|\n|    priya|  pushpa|    USA|     | 29900|    PRIYA|    priya|    Priya|\n|  Karthik|    Subu|    USA|   CA|  6000|  KARTHIK|  karthik|  Karthik|\n|    James|   Smith|    USA|   FL| 20000|    JAMES|    james|    James|\n|   Martin|   Jones|    USA|   CA|  3000|   MARTIN|   martin|   Martin|\n|      Sam|Anderson|     UK|  LND|  8000|      SAM|      sam|      Sam|\n|    Maria| Patrick|     UK|  MCR|  7000|    MARIA|    maria|    Maria|\n|    Robet|   Bevon|     UK|  LND|  3500|    ROBET|    robet|    Robet|\n|    Maria|Anderson|     UK|  MCR|  3000|    MARIA|    maria|    Maria|\n+---------+--------+-------+-----+------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get the Upper case, lower case and title case fro any string\n",
    "# you can apply these functions inside the withColumn method to apply the required transformations\n",
    "\n",
    "# get the upper case for dataframe which using now\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df_uplowcase = df_struct.withColumn(\"UpperCase\",upper(\"firstname\"))\\\n",
    "                        .withColumn(\"LowerCase\",lower(\"firstname\"))\\\n",
    "                        .withColumn(\"TitleCase\",initcap(\"firstname\"))    \n",
    "df_uplowcase.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5ace24-d0b6-46a4-bf8b-a3aab5b9649f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+--------------+\n|firstname|lastname|country|state|  sal|     Full_Name|\n+---------+--------+-------+-----+-----+--------------+\n|     raja|  pushpa|    USA|     |30000|   Raja Pushpa|\n|    priya|  pushpa|    USA|     |29900|  Priya Pushpa|\n|  Karthik|    Subu|    USA|   CA| 6000|  Karthik Subu|\n|    James|   Smith|    USA|   FL|20000|   James Smith|\n|   Martin|   Jones|    USA|   CA| 3000|  Martin Jones|\n|      Sam|Anderson|     UK|  LND| 8000|  Sam Anderson|\n|    Maria| Patrick|     UK|  MCR| 7000| Maria Patrick|\n|    Robet|   Bevon|     UK|  LND| 3500|   Robet Bevon|\n|    Maria|Anderson|     UK|  MCR| 3000|Maria Anderson|\n+---------+--------+-------+-----+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#concat firstname and last name to create a full name\n",
    "#concat(\"col1\",\"col2\",col3..)\n",
    "df2 = df.withColumn(\"Full_Name\",concat(initcap(\"firstname\"),lit(\" \"), initcap(\"lastname\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a14e339b-4fcb-4a3a-8217-518c9da6845e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad32ff9-61d8-4a08-aac6-c2b177018dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----+-----+-------------+\n|firstname|  lastname|country|state|  sal|TrimedVersion|\n+---------+----------+-------+-----+-----+-------------+\n|     raja|pushpa    |    USA|     |30000|       pushpa|\n|    priya|    pushpa|    USA|     |29900|       pushpa|\n|  Karthik|      Subu|    USA|   CA| 6000|         Subu|\n|    James|     Smith|    USA|   FL|20000|        Smith|\n|   Martin|   Jones  |    USA|   CA| 3000|        Jones|\n|      Sam|  Anderson|     UK|  LND| 8000|     Anderson|\n|    Maria|   Patrick|     UK|  MCR| 7000|      Patrick|\n|    Robet|     Bevon|     UK|  LND| 3500|        Bevon|\n|    Maria|  Anderson|     UK|  MCR| 3000|     Anderson|\n+---------+----------+-------+-----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# removing leading and triming spaces\n",
    "# trim function will remove leading and trailing speacs from any string\n",
    "df3 =  df. withColumn(\"TrimedVersion\",trim(\"lastname\"))\n",
    "#df.show()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9a8726-ae12-4e44-8aff-bba5e9280f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "String Function Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edce0211-a7e5-4563-8d38-a6a05fa045af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----+-----+--------+\n|firstname|  lastname|country|state|  sal|      TV|\n+---------+----------+-------+-----+-----+--------+\n|     raja|pushpa    |    USA|     |30000|  pushpa|\n|    priya|    pushpa|    USA|     |29900|  pushpa|\n|  Karthik|      Subu|    USA|   CA| 6000|    Subu|\n|    James|     Smith|    USA|   FL|20000|   Smith|\n|   Martin|   Jones  |    USA|   CA| 3000|   Jones|\n|      Sam|  Anderson|     UK|  LND| 8000|Anderson|\n|    Maria|   Patrick|     UK|  MCR| 7000| Patrick|\n|    Robet|     Bevon|     UK|  LND| 3500|   Bevon|\n|    Maria|  Anderson|     UK|  MCR| 3000|Anderson|\n+---------+----------+-------+-----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_sql= df.withColumn(\"TV\",trim('lastname'))\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682b994e-fb2d-443b-a3d6-fb2103c06c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----+-----+--------------+-------------+------------+---------------+-------------+\n|firstname|  lastname|country|state|  sal|     Full_Name|First4letters|Last4letters|3rdpos_4letters|3rddigitrests|\n+---------+----------+-------+-----+-----+--------------+-------------+------------+---------------+-------------+\n|     raja|pushpa    |    USA|     |30000|   Raja Pushpa|         Raja|        shpa|           ja P|    ja Pushpa|\n|    priya|    pushpa|    USA|     |29900|  Priya Pushpa|         Priy|        shpa|           iya |    iya Pushp|\n|  Karthik|      Subu|    USA|   CA| 6000|  Karthik Subu|         Kart|        Subu|           rthi|    rthik Sub|\n|    James|     Smith|    USA|   FL|20000|   James Smith|         Jame|        mith|           mes |    mes Smith|\n|   Martin|   Jones  |    USA|   CA| 3000|  Martin Jones|         Mart|        ones|           rtin|    rtin Jone|\n|      Sam|  Anderson|     UK|  LND| 8000|  Sam Anderson|         Sam |        rson|           m An|    m Anderso|\n|    Maria|   Patrick|     UK|  MCR| 7000| Maria Patrick|         Mari|        rick|           ria |    ria Patri|\n|    Robet|     Bevon|     UK|  LND| 3500|   Robet Bevon|         Robe|        evon|           bet |    bet Bevon|\n|    Maria|  Anderson|     UK|  MCR| 3000|Maria Anderson|         Mari|        rson|           ria |    ria Ander|\n+---------+----------+-------+-----+-----+--------------+-------------+------------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#get the left right and middle values of any given string\n",
    "#syntax: substring(\"\",,)\n",
    "\n",
    "df_sQL2 = df.withColumn(\"Full_Name\",trim(concat(initcap(\"firstname\"),lit(\" \"),initcap(\"lastname\"))))\n",
    "df_SQL3 = df_sQL2.withColumn(\"First4letters\",substring(\"Full_name\",0,4))\\\n",
    "         .withColumn(\"Last4letters\",substring(\"Full_name\",-4,4))\\\n",
    "         .withColumn(\"3rdpos_4letters\",substring(\"Full_name\",3,4))\\\n",
    "         .withColumn(\"3rddigitrests\",substring(\"Full_name\",3,len(\"Full_name\")))\n",
    "df_SQL3.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Filter_Python 2025-06-23 12:27:28",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}