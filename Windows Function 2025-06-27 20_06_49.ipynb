{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff34843-1b5e-4241-9364-c9bec459260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+\n|  Customer Name|       Category|Sub Category|Sales|\n+---------------+---------------+------------+-----+\n|   Tom Prescott|      Furniture|      Chairs| 4000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|\n|    Joseph Holt|      Furniture|      Tables| 5000|\n|Alejandro Grove|      Furniture| Furnishings| 8000|\n|  Adrian Barton|Office Supplies|     Binders| 3000|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|\n|    Sean Miller|     Technology|    Machines|22000|\n|   Tamara Chand|     Technology|     Copiers|18000|\n|    John Murray|     Technology|      Phones| 5000|\n|Kelly Collister|     Technology| Accessories| 3000|\n+---------------+---------------+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    " #row_number(),rank,dense_rank,lead,lag\n",
    "sampleData = [\n",
    "('Tom Prescott', 'Furniture', 'Chairs', 4000),\n",
    "('Quincy Jones', 'Furniture', 'Bookcases', 4000),\n",
    "('Joseph Holt', 'Furniture', 'Tables', 5000),\n",
    "('Alejandro Grove', 'Furniture', 'Furnishings', 8000),\n",
    "('Adrian Barton', 'Office Supplies', 'Binders', 3000),\n",
    "('Ken Lonsdale', 'Office Supplies', 'Supplies', 9000),\n",
    "('Greg Guthrie', 'Office Supplies', 'Fasteners', 3000),\n",
    "('Yoseph Carroll', 'Office Supplies', 'Storage', 3000),\n",
    "('Sean Miller', 'Technology', 'Machines', 22000),\n",
    "('Tamara Chand', 'Technology', 'Copiers', 18000),\n",
    "('John Murray', 'Technology', 'Phones', 5000),\n",
    "('Kelly Collister', 'Technology', 'Accessories', 3000)\n",
    "]\n",
    "\n",
    "#column names for dataframe\n",
    "columns = [\"Customer Name\",\"Category\",\"Sub Category\",\"Sales\"]\n",
    "\n",
    "#creating the dataframe df\n",
    "\n",
    "df = spark.createDataFrame(data=sampleData, schema=columns )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4f5713-3483-445a-a313-ebbfeedb3c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *  # this has to used when ever you are working with window functions\n",
    "from pyspark.sql.functions import * # this has to used when you re trying to use transformation funcation\n",
    "from pyspark.sql.types import * # this has to be used when you are trying to assign datatypes and struct field and struct type\n",
    "from pyspark.sql.column import * # this has to be used when you are working with col in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d67f9f-620c-4672-9aef-a2f0329ba278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to create a window function in pyspark. there are 2 steps\n",
    "# step1: assign a partition and order level, either table or specific column partition levels\n",
    "# step2: write any function based on above partition created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c26cfc1-5f62-48c0-a39c-4f59429d19a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Example for SQL table level :  over(order by sales des)\n",
    "pysark code for table level: Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "Exaple for SQL partition level: over(partition by category order by sales desc)\n",
    "pyspark code for partiton level: Window.partitionBy(\"Category\").orderB(col(\"Sales\").desc())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2fc7a7-7688-4e3d-a18f-26054aef4e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-------+\n|  Customer Name|       Category|Sub Category|Sales|row_num|\n+---------------+---------------+------------+-----+-------+\n|    Sean Miller|     Technology|    Machines|22000|      1|\n|   Tamara Chand|     Technology|     Copiers|18000|      2|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|      3|\n|Alejandro Grove|      Furniture| Furnishings| 8000|      4|\n|    Joseph Holt|      Furniture|      Tables| 5000|      5|\n|    John Murray|     Technology|      Phones| 5000|      6|\n|   Tom Prescott|      Furniture|      Chairs| 4000|      7|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|      8|\n|  Adrian Barton|Office Supplies|     Binders| 3000|      9|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|     10|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|     11|\n|Kelly Collister|     Technology| Accessories| 3000|     12|\n+---------------+---------------+------------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# create a row_number() for above table based on sales descending\n",
    "\n",
    "#Step1: create a window partition at table level\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "#step2: Creat a Row_number function\n",
    "df2 = df.withColumn(\"row_num\",row_number().over(table_partition))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853862f8-daf0-49db-a037-1e7a5ef4d7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-----+-----+-------+\n|  Customer Name|       Category|Sub Category|Sales|R_num|Rank0|Dns_rak|\n+---------------+---------------+------------+-----+-----+-----+-------+\n|Alejandro Grove|      Furniture| Furnishings| 8000|    1|    1|      1|\n|    Joseph Holt|      Furniture|      Tables| 5000|    2|    2|      2|\n|   Tom Prescott|      Furniture|      Chairs| 4000|    3|    3|      3|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|    4|    3|      3|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|    1|    1|      1|\n|  Adrian Barton|Office Supplies|     Binders| 3000|    2|    2|      2|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|    3|    2|      2|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|    4|    2|      2|\n|    Sean Miller|     Technology|    Machines|22000|    1|    1|      1|\n|   Tamara Chand|     Technology|     Copiers|18000|    2|    2|      2|\n|    John Murray|     Technology|      Phones| 5000|    3|    3|      3|\n|Kelly Collister|     Technology| Accessories| 3000|    4|    4|      4|\n+---------------+---------------+------------+-----+-----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#craete a row_number() for above table based on category partition and sales descending\n",
    "# step: Create a windows partiton at table level\n",
    "\n",
    "window_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2: create a row_number function\n",
    "df2 = df.withColumn(\"R_num\",row_number().over(window_cat))\\\n",
    "        .withColumn(\"Rank0\",rank().over(window_cat))\\\n",
    "        .withColumn(\"Dns_rak\",dense_rank().over(window_cat))    \n",
    "df2.show()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4121ef0d-20dd-4ba9-bc55-919365fcca09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#lead: get the next row value into current row\n",
    "# syntax: lead(\"COLNAME\",<no_of_nextcol>).over(<partition level>)\n",
    "#lag: get the last row value into current row\n",
    "# syntax: lag(\"COLNAME\",<no_of_lastcol>).over(<partition level>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52e2008-ad9a-4dda-89da-ee7da4071c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-----+-----+\n|  Customer Name|       Category|Sub Category|Sales| lag1| lag2|\n+---------------+---------------+------------+-----+-----+-----+\n|    Sean Miller|     Technology|    Machines|22000| NULL| NULL|\n|   Tamara Chand|     Technology|     Copiers|18000|22000| NULL|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|18000|22000|\n|Alejandro Grove|      Furniture| Furnishings| 8000| 9000|18000|\n|    Joseph Holt|      Furniture|      Tables| 5000| 8000| 9000|\n|    John Murray|     Technology|      Phones| 5000| 5000| 8000|\n|   Tom Prescott|      Furniture|      Chairs| 4000| 5000| 5000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000| 4000| 5000|\n|  Adrian Barton|Office Supplies|     Binders| 3000| 4000| 4000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000| 3000| 4000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000| 3000| 3000|\n|Kelly Collister|     Technology| Accessories| 3000| 3000| 3000|\n+---------------+---------------+------------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Get the previous Row Sales into current row\n",
    "#step1: create a window partition at table level\n",
    "\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2: create lag function\n",
    "df2 = df.withColumn(\"lag1\",lag(\"Sales\",1).over(table_partition))\\\n",
    "        .withColumn(\"lag2\",lag(\"Sales\",2).over(table_partition))\n",
    "df2.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e14afc8-6147-4811-8098-6947e73709fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-----+-----+\n|  Customer Name|       Category|Sub Category|Sales|lead1|lead2|\n+---------------+---------------+------------+-----+-----+-----+\n|    Sean Miller|     Technology|    Machines|22000|18000| 9000|\n|   Tamara Chand|     Technology|     Copiers|18000| 9000| 8000|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000| 8000| 5000|\n|Alejandro Grove|      Furniture| Furnishings| 8000| 5000| 5000|\n|    Joseph Holt|      Furniture|      Tables| 5000| 5000| 4000|\n|    John Murray|     Technology|      Phones| 5000| 4000| 4000|\n|   Tom Prescott|      Furniture|      Chairs| 4000| 4000| 3000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000| 3000| 3000|\n|  Adrian Barton|Office Supplies|     Binders| 3000| 3000| 3000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000| 3000| 3000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000| 3000| NULL|\n|Kelly Collister|     Technology| Accessories| 3000| NULL| NULL|\n+---------------+---------------+------------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Get the Next row sales into current row\n",
    "\n",
    "#step1: create a windo partition at table level\n",
    "table_partition = Window.orderBy(col(\"SAles\").desc())\n",
    "\n",
    "#step2: create a lead function\n",
    "\n",
    "df2 = df.withColumn(\"lead1\",lead(\"Sales\",1).over(table_partition))\\\n",
    "        .withColumn(\"lead2\",lead(\"Sales\",2).over(table_partition))\n",
    "df2.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01dd46a4-4148-4f5e-88e5-8998b6452730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+---+\n|  Customer Name|       Category|Sub Category|Sales|R_N|\n+---------------+---------------+------------+-----+---+\n|Alejandro Grove|      Furniture| Furnishings| 8000|  1|\n|    Joseph Holt|      Furniture|      Tables| 5000|  2|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|  1|\n|  Adrian Barton|Office Supplies|     Binders| 3000|  2|\n|    Sean Miller|     Technology|    Machines|22000|  1|\n|   Tamara Chand|     Technology|     Copiers|18000|  2|\n+---------------+---------------+------------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#get the top 2 sales from each category\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "pat_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "df2 = df.withColumn(\"R_N\",row_number().over(pat_cat))\n",
    "df2 = df2.filter(\"R_N <=2\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53fa4c9-58d9-49d0-aa45-c92199633c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----+\n|Cus_ID|Cus_Nmae|      Date|Value|\n+------+--------+----------+-----+\n|     1|     Ram|2025-01-01|    A|\n|     1|     Ram|2025-01-02|    B|\n|     1|     Ram|2025-01-01|    C|\n|     1|     Ram|2025-01-04|    D|\n|     2| Shankar|2025-02-01|    E|\n|     2| Shankar|2025-02-02|    F|\n|     2| Shankar| 2025-02-3|    G|\n|     2| Shankar|2025-02-04|    H|\n|     3|     Raj|2025-03-01|    I|\n|     3|     Raj|2025-03-01|    J|\n|     3|     Raj|2025-03-02|    K|\n|     3|     Raj|2025-03-03|    L|\n|     3|     Raj|2025-03-04|    M|\n|     4|     RaN|2025-04-01|    O|\n|     4|     RaN|2025-04-02|    P|\n|     4|     RaN|2025-04-03|    Q|\n|     4|     RaN|2025-04-04|    R|\n+------+--------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# initialize Spark Session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Remove_Duplicate\").getOrCreate()\n",
    "\n",
    "#Sample data\n",
    "data = [\n",
    "   (1,'Ram','2025-01-01', 'A'),(1,'Ram','2025-01-02', 'B'),(1,'Ram', '2025-01-01', 'C'),\n",
    "   (1,'Ram', '2025-01-04', 'D'),(2,'Shankar', '2025-02-01', 'E'),(2,'Shankar', '2025-02-02', 'F'),\n",
    "   (2,'Shankar', '2025-02-3', 'G'),(2,'Shankar', '2025-02-04', 'H'),(3,'Raj', '2025-03-01','I'),\n",
    "   (3,'Raj', '2025-03-01','J'),(3,'Raj', '2025-03-02','K'),(3,'Raj', '2025-03-03','L'),\n",
    "    (3,'Raj', '2025-03-04','M'),(4,'RaN', '2025-04-01','O'),(4,'RaN', '2025-04-02','P'),\n",
    "    (4,'RaN', '2025-04-03','Q'),(4,'RaN', '2025-04-04','R')\n",
    "\n",
    "] \n",
    "# CREATE dataframe\n",
    "df = spark.createDataFrame(data,[\"Cus_ID\",\"Cus_Nmae\",\"Date\",\"Value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4b986c-e630-4643-ace0-3bc2a4c7fee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----+---+\n|Cus_ID|Cus_Nmae|      Date|Value| RN|\n+------+--------+----------+-----+---+\n|     4|     RaN|2025-04-04|    R|  1|\n|     3|     Raj|2025-03-04|    M|  1|\n|     1|     Ram|2025-01-04|    D|  1|\n|     2| Shankar| 2025-02-3|    G|  1|\n+------+--------+----------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Get last date transformation details for each customer\n",
    "\n",
    "part_cust = Window.partitionBy(\"Cus_Nmae\").orderBy(col(\"Date\").desc())\n",
    "df_ld = df.withColumn(\"RN\",row_number().over(part_cust))\n",
    "df_ld = df_ld.filter(\"RN=1\")\n",
    "df_ld.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3741a232-4e76-4a81-a1b4-de95ef752045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|Salesamt|\n+--------+\n|   10000|\n|   10000|\n|    8000|\n|    8000|\n|    8000|\n|    8000|\n|    2000|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Sample list\n",
    "\n",
    "lsits1 = [\n",
    "    (10000, 'Ram', '2025-01-01','A'),\n",
    "    (10000, 'Ram', '2025-01-02','B'),\n",
    "    (8000, 'Sam', '2025-02-01','D'),\n",
    "    (8000, 'Sam', '2025-02-02','E'),\n",
    "    (8000, 'Sam', '2025-02-03','F'),\n",
    "    (8000, 'Sam', '2025-02-04','G'),\n",
    "    (2000, 'Tom', '2025-03-01','L')\n",
    "]\n",
    "\n",
    "# CRAETE DATAFRAME\n",
    "df = spark.createDataFrame(lsits1,[\"c_id\",\"c_nm\",\"date\",\"value\"])\n",
    "df = df.select(df.c_id.alias(\"Salesamt\"))\n",
    "# show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb247525-d78d-4f5c-8aac-971205e42685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+--------+\n|Salesamt|R_N|rank|Dns_rank|\n+--------+---+----+--------+\n|   10000|  1|   1|       1|\n|   10000|  2|   1|       1|\n|    8000|  3|   3|       2|\n|    8000|  4|   3|       2|\n|    8000|  5|   3|       2|\n|    8000|  6|   3|       2|\n|    2000|  7|   7|       3|\n+--------+---+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "part_rank = Window.orderBy(col(\"Salesamt\").desc())\n",
    "df2 = df.withColumn(\"R_N\",row_number().over(part_rank)).withColumn(\"rank\",rank().over(part_rank)).withColumn(\"Dns_rank\",dense_rank().over(part_rank))\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Windows Function 2025-06-27 20:06:49",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}